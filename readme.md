# Coffee Shop ETL - Arquitectura Medallion en Databricks

Pipeline de datos automatizado para anÃ¡lisis de ventas de cafÃ© implementando arquitectura Medallion (Bronze-Silver-Gold) en Azure Databricks con CI/CD completo.

## ðŸ“‹ Tabla de Contenidos

- [DescripciÃ³n](#descripciÃ³n)
- [Arquitectura](#arquitectura)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [TecnologÃ­as](#tecnologÃ­as)
- [Requisitos Previos](#requisitos-previos)
- [InstalaciÃ³n y ConfiguraciÃ³n](#instalaciÃ³n-y-configuraciÃ³n)
- [Uso](#uso)
- [Modelo de Datos](#modelo-de-datos)
- [CI/CD](#cicd)
- [ConexiÃ³n con Power BI](#conexiÃ³n-con-power-bi)
- [Monitoreo](#monitoreo)
- [Troubleshooting](#troubleshooting)
- [Autor](#autor)

## DescripciÃ³n

Pipeline ETL completo que procesa datos de ventas de una cafeterÃ­a utilizando la arquitectura Medallion en Azure Databricks. El proyecto implementa buenas prÃ¡cticas de ingenierÃ­a de datos con separaciÃ³n de capas, automatizaciÃ³n mediante CI/CD, y entrega de datos listos para anÃ¡lisis de negocio.

## Arquitectura

### Flujo de Datos
CSV (Raw Data)
â†“
Bronze Layer (Ingesta sin transformaciÃ³n)
â†“
Silver Layer (Limpieza + Modelo Dimensional)
â†“
Gold Layer (Agregaciones de Negocio)
â†“
Power BI (VisualizaciÃ³n)

### Capas del Pipeline

#### Bronze Layer
- **PropÃ³sito**: Almacenar datos crudos tal como vienen de la fuente
- **Tabla**: `VENTA_CAFE`
- **Contenido**: Datos sin transformar con timestamp de ingesta

#### Silver Layer
- **PropÃ³sito**: Datos limpios y normalizados en modelo dimensional (Star Schema)
- **Tablas**:
  - `HECHO_VENTAS`: Tabla de hechos con transacciones de venta
  - `DIM_CAFE`: DimensiÃ³n de productos con categorizaciÃ³n
  - `DIM_PAGO`: DimensiÃ³n de mÃ©todos de pago
  - `DIM_FECHA`: DimensiÃ³n calendario para anÃ¡lisis temporal

#### Gold Layer
- **PropÃ³sito**: Agregados optimizados para consumo de negocio
- **Tablas**:
  - `AGG_VENTAS_DIARIAS`: MÃ©tricas diarias
  - `AGG_VENTAS_POR_CAFE`: Performance por producto
  - `AGG_VENTAS_POR_PAGO`: DistribuciÃ³n de mÃ©todos de pago
  - `AGG_VENTAS_POR_DIA_SEMANA`: Patrones semanales
  - `AGG_VENTAS_POR_FRANJA_HORARIA`: AnÃ¡lisis por hora del dÃ­a
  - `AGG_TOP_PRODUCTOS`: Ranking mensual de productos
  - `AGG_RESUMEN_MENSUAL`: Dashboard ejecutivo mensual

## Estructura del Proyecto

coffee-shop-etl/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ databricks-deploy.yml
â”œâ”€â”€ proceso/
â”‚   â”œâ”€â”€ 1-Ddls-Medallion.sql
â”‚   â”œâ”€â”€ 2-Ingest-Coffee-Shop-Data.py
â”‚   â”œâ”€â”€ 3-Transform.py
â”‚   â””â”€â”€ 4-Load.py
â””â”€â”€ README.md


## TecnologÃ­as

- **Azure Databricks**: Motor de procesamiento distribuido
- **Delta Lake**: Formato ACID transaccional
- **PySpark**: Transformaciones de datos
- **Azure Data Lake Storage Gen2**: Almacenamiento persistente
- **GitHub Actions**: CI/CD automatizado
- **Power BI**: VisualizaciÃ³n y anÃ¡lisis

## Requisitos Previos

- Cuenta de Azure con acceso a Databricks
- Workspace de Databricks configurado
- Cluster activo (nombre: `CLUSTER COFFEE SHOP`)
- Cuenta de GitHub
- Azure Data Lake Storage Gen2 configurado
- Power BI Desktop (opcional para visualizaciÃ³n)

## InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio
```bash
git clone https://github.com/tu-usuario/coffee-shop-etl.git
cd coffee-shop-etl

2. Configurar Databricks Personal Access Token

En Databricks: User Settings â†’ Developer â†’ Access Tokens
Click en Generate New Token
Comment: "GitHub CI/CD"
Lifetime: 90 days
Copiar y guardar el token de forma segura

3. Configurar GitHub Secrets
En tu repositorio de GitHub:

Settings â†’ Secrets and variables â†’ Actions
Click en New repository secret
Agregar los siguientes secrets:

Nombre: DATABRICKS_HOST
Valor: https://adb-xxxxxxxxxxxxx.azuredatabricks.net

Nombre: DATABRICKS_TOKEN
Valor: dapi_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

4. Verificar ConfiguraciÃ³n de Storage
Asegurarse de que el storage path estÃ© correctamente configurado:

abfss://coffeeshop@adlsdevluis25.dfs.core.windows.net

Uso
Despliegue AutomÃ¡tico
El pipeline se despliega y ejecuta automÃ¡ticamente al hacer push a master:

git add .
git commit -m "Update pipeline"
git push origin master

GitHub Actions ejecutarÃ¡:

Despliegue de notebooks a /prod/coffee_shop
CreaciÃ³n del workflow CoffeeShopWFDeploy
EjecuciÃ³n completa del pipeline: DDL â†’ Bronze â†’ Silver â†’ Gold
Monitoreo de la ejecuciÃ³n

Despliegue Manual
Para ejecutar manualmente desde GitHub:

Ir al tab Actions en GitHub
Seleccionar Coffee Shop ETL - Databricks Deploy
Click en Run workflow
Seleccionar rama master
Click en Run workflow

EjecuciÃ³n Local en Databricks
Si prefieres ejecutar notebooks manualmente:

Navegar a /prod/coffee_shop en Databricks
Ejecutar notebooks en orden:

1-Ddls-Medallion
2-Ingest-Coffee-Shop-Data
3-Transform
4-Load



Modelo de Datos
Silver Layer - Modelo Dimensional
Tabla de Hechos: HECHO_VENTAS

HECHO_VENTAS
â”œâ”€â”€ fecha: DATE (FK â†’ DIM_FECHA)
â”œâ”€â”€ hora: TIMESTAMP
â”œâ”€â”€ id_cafe: INT (FK â†’ DIM_CAFE)
â”œâ”€â”€ id_pago: INT (FK â†’ DIM_PAGO)
â”œâ”€â”€ cantidad: INT
â”œâ”€â”€ precio_unitario: DOUBLE
â”œâ”€â”€ monto_total: DOUBLE
â”œâ”€â”€ franja_horaria: STRING
â””â”€â”€ fecha_procesamiento: TIMESTAMP



Dimensiones
DIM_CAFE

id_cafe (PK): Identificador Ãºnico del producto
coffee_name: Nombre del cafÃ©
categoria: Espresso, Latte, Bebida FrÃ­a, Otros
tamanio: Grande, Mediano, PequeÃ±o
precio_base: Precio promedio histÃ³rico
activo: Estado del producto
fecha_vigencia: Fecha de alta

DIM_PAGO

id_pago (PK): Identificador del mÃ©todo de pago
cash_type: card, cash, online
proveedor: Visa/Mastercard, PayPal, null
activo: Estado del mÃ©todo

DIM_FECHA

fecha (PK): Fecha completa
anio, trimestre, mes
Month_name, Monthsort
dia_semana, Weekday, Weekdaysort
dia_del_mes
es_fin_semana, es_feriado

Gold Layer - Agregados de Negocio
Todas las tablas Gold incluyen:

MÃ©tricas agregadas especÃ­ficas por dimensiÃ³n
ultima_actualizacion: Timestamp de Ãºltima actualizaciÃ³n
Campos calculados relevantes para anÃ¡lisis

CI/CD
Pipeline de GitHub Actions
El workflow automatiza el ciclo completo:

Deploy: Despliega notebooks desde GitHub a Databricks
Workflow Management: Elimina workflow antiguo si existe
Cluster Lookup: Encuentra el cluster configurado
Workflow Creation: Crea workflow con 4 tareas secuenciales
Execution: Ejecuta pipeline automÃ¡ticamente
Monitoring: Monitorea ejecuciÃ³n cada 30 segundos
Notifications: EnvÃ­a email en caso de error

ConfiguraciÃ³n del Workflow Databricks

Tasks:
â”œâ”€â”€ create_tables_ddl (30min, 1 retry)
â”œâ”€â”€ ingest_bronze (60min, 2 retries)
â”œâ”€â”€ transform_silver (60min, 2 retries)
â””â”€â”€ aggregate_gold (60min, 2 retries)

Schedule: Diario 10:00 AM (Lima)
Timeout total: 4 horas
Max concurrent runs: 1
Email notifications: lchaponant@gmail.com

ConexiÃ³n con Power BI
Prerequisitos

SQL Warehouse activo en Databricks
Personal Access Token generado
Power BI Desktop instalado

Pasos de ConexiÃ³n
1. Obtener Credenciales de Databricks
En Databricks:

SQL Warehouses â†’ Seleccionar tu warehouse
Tab Connection Details
Copiar:

Server hostname: adb-xxxxx.azuredatabricks.net
HTTP Path: /sql/1.0/warehouses/xxxxx



2. Conectar Power BI Desktop
1. Abrir Power BI Desktop
2. Get Data â†’ More â†’ Azure Databricks
3. Ingresar credenciales:
   - Server hostname: [valor copiado]
   - HTTP Path: [valor copiado]
   - Data Connectivity mode: DirectQuery (recomendado)
4. AutenticaciÃ³n:
   - MÃ©todo: Personal Access Token
   - Token: [tu token de Databricks]
5. Click Connect

3. Seleccionar Tablas Gold
En el Navigator, expandir:

catalog_prod
â””â”€â”€ golden
    â”œâ”€â”€ AGG_VENTAS_DIARIAS
    â”œâ”€â”€ AGG_VENTAS_POR_CAFE
    â”œâ”€â”€ AGG_VENTAS_POR_PAGO
    â”œâ”€â”€ AGG_VENTAS_POR_DIA_SEMANA
    â”œâ”€â”€ AGG_VENTAS_POR_FRANJA_HORARIA
    â”œâ”€â”€ AGG_TOP_PRODUCTOS
    â””â”€â”€ AGG_RESUMEN_MENSUAL

Seleccionar las tablas necesarias y click en Load.
4. Configurar Modo de Conectividad
DirectQuery (Recomendado)

Datos siempre actualizados
No ocupa espacio en Power BI
Queries se ejecutan en Databricks

Import Mode

MÃ¡s rÃ¡pido para visualizaciones
Requiere refresh programado
LimitaciÃ³n de volumen de datos

Dashboards Recomendados

Ventas Diarias: Line chart con tendencias temporales
Top Productos: Bar chart con ranking de cafÃ©s
AnÃ¡lisis de Pagos: Pie chart con distribuciÃ³n
Patrones Horarios: Heatmap de ventas por hora
Resumen Ejecutivo: KPIs consolidados con cards

Monitoreo
En Databricks

Workflows:

Ir a Workflows en el menÃº lateral
Buscar CoffeeShopWFDeploy
Ver historial de ejecuciones


Logs por Tarea:

Click en una ejecuciÃ³n especÃ­fica
Click en cada tarea para ver logs detallados
Revisar stdout/stderr en caso de errores



En GitHub Actions

Tab Actions del repositorio
Ver historial de workflows
Click en ejecuciÃ³n especÃ­fica para detalles
Revisar logs de cada step

Notificaciones

Email: Configurado para enviar a lchaponant@gmail.com en caso de fallo
GitHub: Notificaciones en el repositorio


Autor
Luis Alberto Chaponan Tejada
Email: lchaponant@gmail.com
LinkedIn: https://www.linkedin.com/in/luis-chaponan-tejada/
GitHub: ltechdev

Proyecto: Data Engineering - Arquitectura Medallion
TecnologÃ­a: Azure Databricks + Delta Lake + CI/CD
Ãšltima actualizaciÃ³n: 2025
Licencia
MIT License - Ver archivo LICENSE para mÃ¡s detalles