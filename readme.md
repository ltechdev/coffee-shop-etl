<div align="center">

# â˜• Coffee Shop ETL Pipeline
### Arquitectura Medallion en Azure Databricks

[![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://databricks.com/)
[![Azure](https://img.shields.io/badge/Azure-0078D4?style=for-the-badge&logo=microsoft-azure&logoColor=white)](https://azure.microsoft.com/)
[![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)](https://spark.apache.org/)
[![Delta Lake](https://img.shields.io/badge/Delta_Lake-00ADD8?style=for-the-badge&logo=delta&logoColor=white)](https://delta.io/)
[![Power BI](https://img.shields.io/badge/Power_BI-F2C811?style=for-the-badge&logo=power-bi&logoColor=black)](https://powerbi.microsoft.com/)
[![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub_Actions-2088FF?style=for-the-badge&logo=github-actions&logoColor=white)](https://github.com/features/actions)

*Pipeline automatizado de datos para anÃ¡lisis de ventas con arquitectura de tres capas y despliegue continuo*

</div>

---

## ğŸ¯ DescripciÃ³n

Pipeline ETL enterprise-grade que transforma datos crudos de ventas de cafeterÃ­a en insights accionables, implementando la **Arquitectura Medallion** (Bronze-Silver-Gold) en Azure Databricks con **CI/CD completo** y **Delta Lake** para garantizar consistencia ACID.

### âœ¨ CaracterÃ­sticas Principales

- ğŸ”„ **ETL Automatizado** - Pipeline completo con despliegue automÃ¡tico via GitHub Actions
- ğŸ—ï¸ **Arquitectura Medallion** - SeparaciÃ³n clara de capas Bronze â†’ Silver â†’ Gold
- ğŸ“Š **Modelo Dimensional** - Star Schema optimizado para anÃ¡lisis de negocio
- ğŸš€ **CI/CD Integrado** - Deploy automÃ¡tico en cada push a master
- ğŸ“ˆ **Power BI Ready** - ConexiÃ³n directa con SQL Warehouse
- âš¡ **Delta Lake** - ACID transactions y time travel capabilities
- ğŸ”” **Monitoreo** - Notificaciones automÃ¡ticas y logs detallados

---

## ğŸ›ï¸ Arquitectura

### Flujo de Datos

```
ğŸ“„ CSV (Raw Data)
    â†“
ğŸ¥‰ Bronze Layer (Ingesta sin transformaciÃ³n)
    â†“
ğŸ¥ˆ Silver Layer (Limpieza + Modelo Dimensional)
    â†“
ğŸ¥‡ Gold Layer (Agregaciones de Negocio)
    â†“
ğŸ“Š Power BI (VisualizaciÃ³n)
```

### ğŸ“¦ Capas del Pipeline

<table>
<tr>
<td width="33%" valign="top">

#### ğŸ¥‰ Bronze Layer
**PropÃ³sito**: Zona de aterrizaje

**Tabla**: `VENTA_CAFE`

**CaracterÃ­sticas**:
- âœ… Datos tal como vienen de origen
- âœ… Timestamp de ingesta
- âœ… PreservaciÃ³n histÃ³rica
- âœ… Sin validaciones

</td>
<td width="33%" valign="top">

#### ğŸ¥ˆ Silver Layer
**PropÃ³sito**: Modelo dimensional

**Tablas**:
- `HECHO_VENTAS`
- `DIM_CAFE`
- `DIM_PAGO`
- `DIM_FECHA`

**CaracterÃ­sticas**:
- âœ… Star Schema
- âœ… Datos normalizados
- âœ… Claves forÃ¡neas
- âœ… Validaciones completas

</td>
<td width="33%" valign="top">

#### ğŸ¥‡ Gold Layer
**PropÃ³sito**: Analytics-ready

**Tablas**:
- Ventas diarias
- Top productos
- AnÃ¡lisis temporal
- KPIs ejecutivos

**CaracterÃ­sticas**:
- âœ… Pre-agregados
- âœ… Optimizado para BI
- âœ… Performance mÃ¡ximo
- âœ… Actualizaciones automÃ¡ticas

</td>
</tr>
</table>

---

## ğŸ“ Estructura del Proyecto

```
coffee-shop-etl/
â”‚
â”œâ”€â”€ ğŸ“‚ .github/
â”‚   â””â”€â”€ ğŸ“‚ workflows/
â”‚       â””â”€â”€ ğŸ“„ databricks-deploy.yml    # Pipeline CI/CD
â”‚
â”œâ”€â”€ ğŸ“‚ proceso/
â”‚   â”œâ”€â”€ ğŸ“„ 1-Ddls-Medallion.sql         # CreaciÃ³n de esquema
â”‚   â”œâ”€â”€ ğŸ 2-Ingest-Coffee-Shop-Data.py # Bronze Layer
â”‚   â”œâ”€â”€ ğŸ 3-Transform.py                # Silver Layer
â”‚   â””â”€â”€ ğŸ 4-Load.py                     # Gold Layer
â”‚
â””â”€â”€ ğŸ“„ README.md
```

---

## ğŸ› ï¸ TecnologÃ­as

<div align="center">

| TecnologÃ­a | PropÃ³sito |
|:----------:|:----------|
| ![Databricks](https://img.shields.io/badge/Azure_Databricks-FF3621?style=flat-square&logo=databricks&logoColor=white) | Motor de procesamiento distribuido Spark |
| ![Delta Lake](https://img.shields.io/badge/Delta_Lake-00ADD8?style=flat-square&logo=delta&logoColor=white) | Storage layer con ACID transactions |
| ![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=flat-square&logo=apache-spark&logoColor=white) | Framework de transformaciÃ³n de datos |
| ![ADLS](https://img.shields.io/badge/ADLS_Gen2-0078D4?style=flat-square&logo=microsoft-azure&logoColor=white) | Data Lake para almacenamiento persistente |
| ![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-2088FF?style=flat-square&logo=github-actions&logoColor=white) | AutomatizaciÃ³n CI/CD |
| ![Power BI](https://img.shields.io/badge/Power_BI-F2C811?style=flat-square&logo=power-bi&logoColor=black) | Business Intelligence y visualizaciÃ³n |

</div>

---

## âš™ï¸ Requisitos Previos

- â˜ï¸ Cuenta de Azure con acceso a Databricks
- ğŸ’» Workspace de Databricks configurado
- ğŸ–¥ï¸ Cluster activo (nombre: `CLUSTER COFFEE SHOP`)
- ğŸ™ Cuenta de GitHub con permisos de administrador
- ğŸ“¦ Azure Data Lake Storage Gen2 configurado
- ğŸ“Š Power BI Desktop (opcional para visualizaciÃ³n)

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1ï¸âƒ£ Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/coffee-shop-etl.git
cd coffee-shop-etl
```

### 2ï¸âƒ£ Configurar Databricks Token

1. Ir a Databricks Workspace
2. **User Settings** â†’ **Developer** â†’ **Access Tokens**
3. Click en **Generate New Token**
4. Configurar:
   - **Comment**: `GitHub CI/CD`
   - **Lifetime**: `90 days`
5. âš ï¸ Copiar y guardar el token

### 3ï¸âƒ£ Configurar GitHub Secrets

En tu repositorio: **Settings** â†’ **Secrets and variables** â†’ **Actions**

| Secret Name | Valor Ejemplo |
|------------|---------------|
| `DATABRICKS_HOST` | `https://adb-xxxxx.azuredatabricks.net` |
| `DATABRICKS_TOKEN` | `dapi_xxxxxxxxxxxxxxxx` |

### 4ï¸âƒ£ Verificar Storage Configuration

```python
storage_path = "abfss://coffeeshop@adlsdevluis25.dfs.core.windows.net"
```

<div align="center">

âœ… **Â¡ConfiguraciÃ³n completa!**

</div>

---

## ğŸ’» Uso

### ğŸ”„ Despliegue AutomÃ¡tico (Recomendado)

```bash
git add .
git commit -m "âœ¨ feat: mejoras en pipeline"
git push origin master
```

**GitHub Actions ejecutarÃ¡**:
- ğŸ“¤ Deploy de notebooks a `/prod/coffee_shop`
- ğŸ”§ CreaciÃ³n del workflow `CoffeeShopWFDeploy`
- â–¶ï¸ EjecuciÃ³n completa: DDL â†’ Bronze â†’ Silver â†’ Gold
- ğŸ“§ Notificaciones de resultados

### ğŸ–±ï¸ Despliegue Manual desde GitHub

1. Ir al tab **Actions** en GitHub
2. Seleccionar **Coffee Shop ETL - Databricks Deploy**
3. Click en **Run workflow**
4. Seleccionar rama `master`
5. Click en **Run workflow**

### ğŸ”§ EjecuciÃ³n Local en Databricks

Navegar a `/prod/coffee_shop` y ejecutar en orden:

```
1ï¸âƒ£ 1-Ddls-Medallion.sql         â†’ Crear esquema
2ï¸âƒ£ 2-Ingest-Coffee-Shop-Data.py â†’ Bronze Layer
3ï¸âƒ£ 3-Transform.py                â†’ Silver Layer
4ï¸âƒ£ 4-Load.py                     â†’ Gold Layer
```

---

## ğŸ“Š Modelo de Datos

### ğŸ¥ˆ Silver Layer - Star Schema

```
                    DIM_FECHA
                        |
                        |
DIM_CAFE -------- HECHO_VENTAS -------- DIM_PAGO
                        |
                   (Fact Table)
```

#### ğŸ¯ HECHO_VENTAS (Fact Table)

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `fecha` | DATE | Fecha de transacciÃ³n (FK) |
| `hora` | TIMESTAMP | Hora exacta de venta |
| `id_cafe` | INT | Producto vendido (FK) |
| `id_pago` | INT | MÃ©todo de pago (FK) |
| `cantidad` | INT | Unidades vendidas |
| `precio_unitario` | DOUBLE | Precio por unidad |
| `monto_total` | DOUBLE | Total de transacciÃ³n |
| `franja_horaria` | STRING | MaÃ±ana/Tarde/Noche |
| `fecha_procesamiento` | TIMESTAMP | Audit timestamp |

#### â˜• DIM_CAFE (Product Dimension)

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `id_cafe` | INT | Clave primaria |
| `coffee_name` | STRING | Nombre del producto |
| `categoria` | STRING | Espresso, Latte, Bebida FrÃ­a, Otros |
| `tamanio` | STRING | Grande, Mediano, PequeÃ±o |
| `precio_base` | DOUBLE | Precio promedio histÃ³rico |
| `activo` | BOOLEAN | Estado del producto |
| `fecha_vigencia` | DATE | Fecha de alta |

#### ğŸ’³ DIM_PAGO (Payment Dimension)

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `id_pago` | INT | Clave primaria |
| `cash_type` | STRING | card, cash, online |
| `proveedor` | STRING | Visa/Mastercard, PayPal, null |
| `activo` | BOOLEAN | Estado del mÃ©todo |

#### ğŸ“… DIM_FECHA (Date Dimension)

| Campo | Tipo | DescripciÃ³n |
|-------|------|-------------|
| `fecha` | DATE | Fecha completa (PK) |
| `anio` | INT | AÃ±o |
| `trimestre` | INT | Trimestre (1-4) |
| `mes` | INT | Mes (1-12) |
| `month_name` | STRING | Nombre del mes |
| `dia_semana` | INT | DÃ­a de la semana (1-7) |
| `weekday` | STRING | Nombre del dÃ­a |
| `es_fin_semana` | BOOLEAN | SÃ¡bado o domingo |
| `es_feriado` | BOOLEAN | DÃ­a feriado |

### ğŸ¥‡ Gold Layer - Agregados de Negocio

| Tabla | DescripciÃ³n | ActualizaciÃ³n |
|-------|-------------|---------------|
| `AGG_VENTAS_DIARIAS` | MÃ©tricas diarias consolidadas | Diaria |
| `AGG_VENTAS_POR_CAFE` | Performance por producto | Diaria |
| `AGG_VENTAS_POR_PAGO` | DistribuciÃ³n de mÃ©todos de pago | Diaria |
| `AGG_VENTAS_POR_DIA_SEMANA` | Patrones semanales | Diaria |
| `AGG_VENTAS_POR_FRANJA_HORARIA` | AnÃ¡lisis por hora del dÃ­a | Diaria |
| `AGG_TOP_PRODUCTOS` | Ranking mensual de productos | Mensual |
| `AGG_RESUMEN_MENSUAL` | Dashboard ejecutivo | Mensual |

---

## ğŸ”„ CI/CD

### Pipeline de GitHub Actions

```yaml
Workflow: Coffee Shop ETL - Databricks Deploy
â”œâ”€â”€ Deploy notebooks â†’ /prod/coffee_shop
â”œâ”€â”€ Eliminar workflow antiguo (si existe)
â”œâ”€â”€ Buscar cluster configurado
â”œâ”€â”€ Crear nuevo workflow con 4 tareas
â”œâ”€â”€ Ejecutar pipeline automÃ¡ticamente
â””â”€â”€ Monitorear y notificar resultados
```

### ConfiguraciÃ³n del Workflow Databricks

```
Tasks:
â”œâ”€â”€ create_tables_ddl    (30min, 1 retry)
â”œâ”€â”€ ingest_bronze        (60min, 2 retries)
â”œâ”€â”€ transform_silver     (60min, 2 retries)
â””â”€â”€ aggregate_gold       (60min, 2 retries)

â° Schedule: Diario 10:00 AM (Lima)
â±ï¸ Timeout total: 4 horas
ğŸ”’ Max concurrent runs: 1
ğŸ“§ Notificaciones: lchaponant@gmail.com
```

---

## ğŸ“ˆ ConexiÃ³n con Power BI

### Prerequisitos

- âœ… SQL Warehouse activo en Databricks
- âœ… Personal Access Token generado
- âœ… Power BI Desktop instalado

### Pasos de ConexiÃ³n

#### 1ï¸âƒ£ Obtener Credenciales de Databricks

En Databricks: **SQL Warehouses** â†’ Seleccionar warehouse â†’ **Connection Details**

Copiar:
- `Server hostname`: `adb-xxxxx.azuredatabricks.net`
- `HTTP Path`: `/sql/1.0/warehouses/xxxxx`

#### 2ï¸âƒ£ Conectar Power BI Desktop

1. Abrir Power BI Desktop
2. **Get Data** â†’ **More** â†’ **Azure Databricks**
3. Ingresar credenciales copiadas
4. **Data Connectivity mode**: `DirectQuery` (recomendado)
5. **AutenticaciÃ³n**: Personal Access Token
6. Click **Connect**

#### 3ï¸âƒ£ Seleccionar Tablas Gold

```
catalog_prod
â””â”€â”€ golden
    â”œâ”€â”€ AGG_VENTAS_DIARIAS
    â”œâ”€â”€ AGG_VENTAS_POR_CAFE
    â”œâ”€â”€ AGG_VENTAS_POR_PAGO
    â”œâ”€â”€ AGG_VENTAS_POR_DIA_SEMANA
    â”œâ”€â”€ AGG_VENTAS_POR_FRANJA_HORARIA
    â”œâ”€â”€ AGG_TOP_PRODUCTOS
    â””â”€â”€ AGG_RESUMEN_MENSUAL
```

#### 4ï¸âƒ£ Configurar Modo de Conectividad

**DirectQuery (Recomendado)**
- âœ… Datos siempre actualizados
- âœ… No ocupa espacio en Power BI
- âœ… Queries se ejecutan en Databricks

**Import Mode**
- âœ… MÃ¡s rÃ¡pido para visualizaciones
- âš ï¸ Requiere refresh programado
- âš ï¸ LimitaciÃ³n de volumen de datos

### ğŸ“Š Dashboards Recomendados

- ğŸ“ˆ **Ventas Diarias**: Line chart con tendencias temporales
- ğŸ† **Top Productos**: Bar chart con ranking de cafÃ©s
- ğŸ’³ **AnÃ¡lisis de Pagos**: Pie chart con distribuciÃ³n
- â° **Patrones Horarios**: Heatmap de ventas por hora
- ğŸ“Š **Resumen Ejecutivo**: KPIs consolidados con cards

---

## ğŸ” Monitoreo

### En Databricks

**Workflows**:
- Ir a **Workflows** en el menÃº lateral
- Buscar `CoffeeShopWFDeploy`
- Ver historial de ejecuciones

**Logs por Tarea**:
- Click en una ejecuciÃ³n especÃ­fica
- Click en cada tarea para ver logs detallados
- Revisar stdout/stderr en caso de errores

### En GitHub Actions

- Tab **Actions** del repositorio
- Ver historial de workflows
- Click en ejecuciÃ³n especÃ­fica para detalles
- Revisar logs de cada step

### Notificaciones

- ğŸ“§ **Email**: Configurado para `lchaponant@gmail.com` en caso de fallo
- ğŸ”” **GitHub**: Notificaciones en el repositorio

---

## ğŸ› Troubleshooting

<details>
<summary><b>Error: Cluster not found</b></summary>

**SoluciÃ³n**: Verificar que el cluster `CLUSTER COFFEE SHOP` estÃ© activo en Databricks.

```bash
# Verificar nombre exacto del cluster en Databricks
```
</details>

<details>
<summary><b>Error: Authentication failed</b></summary>

**SoluciÃ³n**: Regenerar Personal Access Token y actualizar GitHub Secrets.
</details>

<details>
<summary><b>Error: Storage path not found</b></summary>

**SoluciÃ³n**: Verificar que el ADLS Gen2 estÃ© montado correctamente:

```python
storage_path = "abfss://coffeeshop@adlsdevluis25.dfs.core.windows.net"
```
</details>

---

## ğŸ‘¤ Autor

<div align="center">

### Luis Alberto Chaponan Tejada

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/luis-chaponan-tejada/)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ltechdev)
[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:lchaponant@gmail.com)

**Data Engineering** | **Azure Databricks** | **Delta Lake** | **CI/CD**

</div>

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

<div align="center">

**Proyecto**: Data Engineering - Arquitectura Medallion  
**TecnologÃ­a**: Azure Databricks + Delta Lake + CI/CD  
**Ãšltima actualizaciÃ³n**: 2025


</div>